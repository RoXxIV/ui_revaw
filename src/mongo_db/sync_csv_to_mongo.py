#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de synchronisation CSV vers MongoDB - Version am√©lior√©e
Con√ßu pour fonctionner via CRON la nuit, hors production.
"""

import os
import sys
import csv
import json
import time
import glob
from datetime import datetime
from pymongo import MongoClient, UpdateOne
from pymongo.errors import (ConnectionFailure, ServerSelectionTimeoutError, BulkWriteError, PyMongoError)

# üìå Configuration
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CSV_PATH = os.path.join(SCRIPT_DIR, '../../printed_serials.csv')
DATA_FOLDER = os.path.join(SCRIPT_DIR, '../../data')
CONFIG_FILE = os.path.join(SCRIPT_DIR, 'mongo_config.json')

# Configuration retry et timeouts
MAX_RETRIES = 3
RETRY_DELAY = 5  # secondes
CONNECTION_TIMEOUT = 10  # secondes
SERVER_SELECTION_TIMEOUT = 5  # secondes

IGNORED_FOLDERS = {'archive_fails'}
BANC_FOLDERS = [f"banc{i}" for i in range(1, 5)]


def log_message(message, level="INFO"):
    """Simple logging avec timestamp vers console."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")


def log_to_file(message, level="INFO"):
    """Log uniquement les √©v√©nements importants dans un fichier."""
    # Seulement les niveaux importants dans le fichier
    if level not in ["INFO", "WARNING", "ERROR"]:
        return

    log_file = os.path.join(SCRIPT_DIR, "mongo_sync.log")
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    try:
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] [{level}] {message}\n")
    except Exception:
        # Si on peut pas √©crire dans le log, on continue silencieusement
        pass


def load_config():
    """Charge la configuration MongoDB avec gestion d'erreurs."""
    try:
        with open(CONFIG_FILE, "r") as f:
            config = json.load(f)
        log_message("Configuration MongoDB charg√©e avec succ√®s")
        return config
    except FileNotFoundError:
        log_message(f"Fichier de configuration non trouv√©: {CONFIG_FILE}", "ERROR")
        sys.exit(1)
    except json.JSONDecodeError as e:
        log_message(f"Erreur JSON dans la configuration: {e}", "ERROR")
        sys.exit(1)


def create_mongo_client_with_retry(config):
    """Cr√©e une connexion MongoDB avec retry et timeouts optimis√©s."""
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            log_message(f"Tentative de connexion MongoDB ({attempt}/{MAX_RETRIES})")

            client = MongoClient(
                config["MONGO_URI"],
                serverSelectionTimeoutMS=SERVER_SELECTION_TIMEOUT * 1000,
                connectTimeoutMS=CONNECTION_TIMEOUT * 1000,
                socketTimeoutMS=30000,  # 30s pour les op√©rations
                retryWrites=True,
                retryReads=True)

            # Test de connexion avec ping
            client.admin.command('ping')
            log_message("Connexion MongoDB √©tablie avec succ√®s")
            log_to_file("Connexion MongoDB √©tablie avec succ√®s")
            return client

        except ServerSelectionTimeoutError:
            log_message(f"Timeout serveur MongoDB (tentative {attempt})", "WARNING")
        except ConnectionFailure as e:
            log_message(f"√âchec connexion MongoDB: {e} (tentative {attempt})", "WARNING")
        except Exception as e:
            log_message(f"Erreur inattendue MongoDB: {e} (tentative {attempt})", "ERROR")

        if attempt < MAX_RETRIES:
            wait_time = RETRY_DELAY * attempt  # Backoff progressif
            log_message(f"Retry dans {wait_time} secondes...")
            time.sleep(wait_time)

    log_message("Impossible de se connecter √† MongoDB apr√®s toutes les tentatives", "ERROR")
    log_to_file("Impossible de se connecter √† MongoDB apr√®s toutes les tentatives", "ERROR")
    sys.exit(1)


def parse_iso_safe(timestamp_str):
    """Parse un timestamp ISO avec gestion d'erreurs."""
    if not timestamp_str:
        return None
    try:
        return datetime.fromisoformat(timestamp_str)
    except (ValueError, TypeError):
        log_message(f"Timestamp invalide ignor√©: {timestamp_str}", "WARNING")
        return None


def get_config_data_for_serial(serial):
    """Recherche les donn√©es config.json pour un num√©ro de s√©rie."""
    for banc in BANC_FOLDERS:
        if banc in IGNORED_FOLDERS:
            continue

        banc_path = os.path.join(DATA_FOLDER, banc)
        if not os.path.isdir(banc_path):
            continue

        pattern = os.path.join(banc_path, f"*-{serial}")
        matching_dirs = glob.glob(pattern)

        for match in matching_dirs:
            config_path = os.path.join(match, "config.json")
            if os.path.isfile(config_path):
                try:
                    with open(config_path, "r", encoding="utf-8") as f:
                        return json.load(f)
                except Exception as e:
                    log_message(f"Erreur lecture config pour {serial}: {e}", "WARNING")
    return None


def validate_csv_file():
    """Valide l'existence et l'accessibilit√© du fichier CSV."""
    if not os.path.exists(CSV_PATH):
        log_message(f"Fichier CSV non trouv√©: {CSV_PATH}", "ERROR")
        sys.exit(1)

    try:
        with open(CSV_PATH, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            header = next(reader, None)
            if not header:
                log_message("Fichier CSV vide ou sans en-t√™te", "ERROR")
                sys.exit(1)

            required_columns = ["NumeroSerie", "TimestampImpression", "CodeAleatoireQR"]
            missing_columns = [col for col in required_columns if col not in header]
            if missing_columns:
                log_message(f"Colonnes manquantes dans le CSV: {missing_columns}", "ERROR")
                sys.exit(1)

        log_message("Validation du fichier CSV r√©ussie")

    except Exception as e:
        log_message(f"Erreur lors de la validation du CSV: {e}", "ERROR")
        sys.exit(1)


def process_csv_with_error_handling(collection):
    """Traite le CSV avec gestion d'erreurs robuste."""
    successful_operations = 0
    failed_operations = 0

    try:
        with open(CSV_PATH, newline='', encoding='utf-8') as csvfile:
            reader = csv.DictReader(csvfile)
            operations = []
            batch_size = 100  # Traitement par lots

            for row_num, row in enumerate(reader, 1):
                try:
                    # Construction du document
                    doc = {
                        "TimestampImpression": parse_iso_safe(row.get("TimestampImpression")),
                        "NumeroSerie": row.get("NumeroSerie", "").strip(),
                        "CodeAleatoireQR": row.get("CodeAleatoireQR", "").strip(),
                        "TimestampTestDone": parse_iso_safe(row.get("TimestampTestDone")),
                        "TimestampExpedition": parse_iso_safe(row.get("TimestampExpedition")),
                        "version": row.get("version", "").strip()
                    }

                    # Validation basique
                    if not doc["NumeroSerie"]:
                        log_message(f"Ligne {row_num}: NumeroSerie vide, ignor√©e", "WARNING")
                        failed_operations += 1
                        continue

                    # Enrichissement avec config.json
                    config_data = get_config_data_for_serial(doc["NumeroSerie"])
                    if config_data:
                        for field in [
                                "capacity_ah", "capacity_wh", "ri_discharge_average", "ri_charge_average",
                                "diffusion_discharge_average", "diffusion_charge_average", "delta_ri_cells",
                                "delta_diffusion_cells"
                        ]:
                            if field in config_data:
                                doc[field] = config_data[field]

                    # Ajout √† la batch
                    operations.append(UpdateOne({"NumeroSerie": doc["NumeroSerie"]}, {"$set": doc}, upsert=True))

                    # Ex√©cution par lots
                    if len(operations) >= batch_size:
                        successful_batch, failed_batch = execute_batch_operations(collection, operations)
                        successful_operations += successful_batch
                        failed_operations += failed_batch
                        operations = []

                except Exception as e:
                    log_message(f"Erreur traitement ligne {row_num}: {e}", "WARNING")
                    failed_operations += 1
                    continue

            # Traitement du dernier lot
            if operations:
                successful_batch, failed_batch = execute_batch_operations(collection, operations)
                successful_operations += successful_batch
                failed_operations += failed_batch

    except Exception as e:
        log_message(f"Erreur critique lors du traitement du CSV: {e}", "ERROR")
        return successful_operations, failed_operations + 1

    return successful_operations, failed_operations


def execute_batch_operations(collection, operations):
    """Ex√©cute une batch d'op√©rations avec gestion d'erreurs."""
    try:
        result = collection.bulk_write(operations, ordered=False)
        successful = result.upserted_count + result.modified_count
        return successful, 0

    except BulkWriteError as bwe:
        # Gestion des erreurs partielles dans les bulk operations
        result = bwe.details
        successful = result.get('nUpserted', 0) + result.get('nModified', 0)
        failed = len(result.get('writeErrors', []))

        log_message(f"Bulk write partiel: {successful} r√©ussies, {failed} √©chou√©es", "WARNING")
        log_to_file(f"Bulk write partiel: {successful} r√©ussies, {failed} √©chou√©es", "WARNING")

        # Log des erreurs sp√©cifiques (limit√© pour √©viter le spam)
        for error in result.get('writeErrors', [])[:3]:  # Max 3 erreurs logg√©es
            log_message(f"Erreur write: {error.get('errmsg', 'Erreur inconnue')}", "WARNING")

        return successful, failed

    except Exception as e:
        log_message(f"Erreur lors de l'ex√©cution batch: {e}", "ERROR")
        return 0, len(operations)


def create_indexes_safely(collection):
    """Cr√©e les index de mani√®re s√©curis√©e."""
    try:
        # Index unique sur NumeroSerie (si pas d√©j√† existant)
        existing_indexes = collection.list_indexes()
        index_names = [idx['name'] for idx in existing_indexes]

        if 'NumeroSerie_1' not in index_names:
            collection.create_index("NumeroSerie", unique=True)
            log_message("Index unique cr√©√© sur NumeroSerie")
        else:
            log_message("Index sur NumeroSerie d√©j√† existant")

    except Exception as e:
        log_message(f"Erreur cr√©ation index: {e}", "WARNING")


def print_summary(successful, failed, duration):
    """Affiche un r√©sum√© de l'op√©ration."""
    total = successful + failed
    success_rate = (successful / total * 100) if total > 0 else 0

    summary_lines = [
        "=" * 50, "R√âSUM√â DE LA SYNCHRONISATION", "=" * 50, f"üìÑ Op√©rations totales: {total}", f"‚úÖ Succ√®s: {successful}",
        f"‚ùå √âchecs: {failed}", f"üìä Taux de r√©ussite: {success_rate:.1f}%", f"‚è±Ô∏è  Dur√©e: {duration:.2f} secondes",
        "=" * 50
    ]

    # Affichage console
    for line in summary_lines:
        log_message(line)

    # Log fichier (r√©sum√© compact)
    log_to_file(
        f"Synchronisation termin√©e: {successful} succ√®s, {failed} √©checs, {success_rate:.1f}% r√©ussite, {duration:.2f}s"
    )


def main():
    """Fonction principale avec gestion d'erreurs compl√®te."""
    start_time = time.time()

    try:
        log_message("üöÄ D√©but de la synchronisation CSV -> MongoDB")
        log_to_file("D√©but de la synchronisation CSV -> MongoDB")

        # Validation des pr√©requis
        log_message("üìã Validation des pr√©requis...")
        config = load_config()
        validate_csv_file()

        # Connexion MongoDB
        client = create_mongo_client_with_retry(config)

        try:
            db = client[config["DB_NAME"]]
            collection = db[config["COLLECTION_NAME"]]

            # Cr√©ation des index
            create_indexes_safely(collection)

            # Traitement principal
            log_message("üìä D√©but du traitement des donn√©es...")
            successful, failed = process_csv_with_error_handling(collection)

            # R√©sum√©
            duration = time.time() - start_time
            print_summary(successful, failed, duration)

            # Code de sortie selon les r√©sultats
            if failed == 0:
                log_message("‚úÖ Synchronisation termin√©e avec succ√®s")
                log_to_file("Synchronisation termin√©e avec succ√®s")
                sys.exit(0)
            elif successful > 0:
                log_message("‚ö†Ô∏è  Synchronisation termin√©e avec des erreurs partielles")
                log_to_file("Synchronisation termin√©e avec des erreurs partielles", "WARNING")
                sys.exit(0)  # Succ√®s partiel = OK pour CRON
            else:
                log_message("‚ùå Synchronisation √©chou√©e")
                log_to_file("Synchronisation √©chou√©e - aucune op√©ration r√©ussie", "ERROR")
                sys.exit(1)

        finally:
            client.close()
            log_message("üîå Connexion MongoDB ferm√©e")

    except KeyboardInterrupt:
        log_message("‚èπÔ∏è  Arr√™t demand√© par l'utilisateur", "WARNING")
        sys.exit(130)
    except Exception as e:
        log_message(f"üí• Erreur critique non g√©r√©e: {e}", "ERROR")
        log_to_file(f"Erreur critique non g√©r√©e: {e}", "ERROR")
        sys.exit(1)


if __name__ == "__main__":
    main()
